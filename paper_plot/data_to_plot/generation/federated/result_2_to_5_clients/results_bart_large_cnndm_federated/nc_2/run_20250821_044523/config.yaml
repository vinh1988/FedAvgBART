batch_size: 8
clients_per_round: 2
data_dir: ./data/cnndm
dirichlet_alpha: 0.5
eval_steps: 500
learning_rate: 5e-5
length_penalty: 2.0
local_epochs: 1
log_interval: 10
logging_steps: 100
max_grad_norm: 1.0
max_source_length: 256
max_target_length: 64
max_test_samples: 100
max_train_samples: 10000
max_val_samples: 5000
model_name: facebook/bart-large-cnn
no_repeat_ngram_size: 3
num_beams: 4
num_clients: 2
num_rounds: 2
output_dir: ./results_distilbart_cnndm_federated
save_steps: 1000
seed: 42
test_split: test
train_split: train
use_wandb: true
val_split: validation
warmup_steps: 500
weight_decay: 0.01
