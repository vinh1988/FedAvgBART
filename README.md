# A Federated Learning Approach using Pre-trained BART Model for Text Classification and Generation Tasks
![Made With python](https://img.shields.io/badge/Made%20with-Python-brightgreen)![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)![Open Source Love svg1](https://badges.frapsoft.com/os/v1/open-source.svg?v=103)![Pytorch](https://img.shields.io/badge/Made%20with-Pytorch-green.svg)

### Abstract
Federated Learning (FL) facilitates collaborative model training across decentralized datasets while safeguarding data privacy, making it highly suitable for Natural Language Processing (NLP) applications involving sensitive information. This research explores the federated fine-tuning of pre-trained BART-large and DistilBART models for text classification and generation tasks. Through comprehensive experiments on the 20News and CNN/DailyMail datasets, we simulate diverse, heterogeneous settings with varying client counts and non-independent and identically distributed (non-IID) data configurations. Our results demonstrate that federated training can surpass centralized fine-tuning in performance. Specifically, BART-large exhibits exceptional proficiency in classification tasks, while DistilBART excels in text generation, offering superior computational efficiency for resource-limited clients. Furthermore, BART-large maintains greater stability across diverse client scales, whereas non-IID data disproportionately affects smaller models, underscoring the robustness of larger architectures. These insights highlight the task-specific advantages of integrating FL with BART-based models and provide actionable guidance for deploying large language models in privacy-preserving, distributed NLP frameworks.

### Paper
(Link paper will be deployed)

### Citation
```
@inproceedings{pham2025federated,
  title={A Federated Learning Approach using Pre-trained BART Model for Text Classification and Generation Tasks},
  author={Pham, Quang-Vinh and Le, Xuan-Tuyen and Le, Quang-Hung},
  year={2025}
}
```
